{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEE9y9x4Qmsu",
        "outputId": "143956e2-30a0-4525-cf92-0868a7c48455"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrhVC0dRRF8w",
        "outputId": "0c60051d-2a06-4cbb-a5a1-3a83e0a27845"
      },
      "source": [
        "import codecs\r\n",
        "import string\r\n",
        "import os\r\n",
        "import nltk\r\n",
        "import re\r\n",
        "import ast \r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAffmImXRHSQ"
      },
      "source": [
        "#Function is called for preprocessed the data\r\n",
        "def preprocessing(texts):\r\n",
        "    #including words only removing all the non-letters and numbers\r\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\",  # Search for all non-letters\r\n",
        "                          \" \",          # Replace all non-letters with spaces\r\n",
        "                          texts)\r\n",
        "    #word Tokenization\r\n",
        "    word_tokens = word_tokenize(letters_only)\r\n",
        "    #Lemmatization and removing stopwords\r\n",
        "    lemmatizer = WordNetLemmatizer()\r\n",
        "    word_lemm = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stopwords.words('english')]\r\n",
        "    return word_lemm\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWdKtc9nRRiG"
      },
      "source": [
        "#Function is called to read all the files and ignoring FARNON,SRE,index,and all \".\" files like \".header\" \r\n",
        "def preprocessing_data(files):\r\n",
        "    #Creating a dictonary of preprocessedData this dictionary contains the file name as key and preprocessed data as values.\r\n",
        "    preprocessedData = {}\r\n",
        "    i = 0\r\n",
        "    for file in files:\r\n",
        "        #Ignoring FARNON,SRE,index,and all \".\" files like \".header\"     \r\n",
        "        if(file != 'FARNON' and file != 'SRE' and file != 'whgdsreg' and file != \".header\" and file != \".musings\" and file != \".descs\" and file != \"index.html\"):\r\n",
        "            with open('/content/drive/My Drive/IRE/stories/'+file, mode='r',errors=\"ignore\",encoding = \"ISO-8859-1\") as f:\r\n",
        "                #Sending the data to the preprocessing in lowercase\r\n",
        "                preprocessedData[file] = preprocessing(f.read().lower())\r\n",
        "                i+=1\r\n",
        "                print(file + \" has processed \",i)\r\n",
        "    return preprocessedData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEBb2jso0aqJ"
      },
      "source": [
        "# {word : {key1:count1, key2:count2}, word2:{key1:count1}}\r\n",
        "#Here key is word and value is a dictionary in which key is the document file and count is the word frequency in that document\r\n",
        "indexing = {}\r\n",
        "#first we have created a dictionary of indexing\r\n",
        "def createIndexing(wordList, key):\r\n",
        "  #apply loop for each word\r\n",
        "  for word in wordList:\r\n",
        "    #checking if that word is present in indexing\r\n",
        "    if word in indexing:\r\n",
        "      #checking if the word is already in a specific document, then increment its count\r\n",
        "      if key in indexing[word]:\r\n",
        "        indexing[word][key] += 1\r\n",
        "      #otherwose specify the document by 1\r\n",
        "      else:\r\n",
        "        indexing[word][key] = 1\r\n",
        "    #the word is not present in indexing \r\n",
        "    else:\r\n",
        "      indexing[word] = {}\r\n",
        "      indexing[word][key] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4JUQOFl303n"
      },
      "source": [
        "invertedIndexing, postingList = {}, {}\r\n",
        "def createInvertedIndexing():\r\n",
        "  #starting the index from 101\r\n",
        "  index = 101\r\n",
        "  for word, value in indexing.items():\r\n",
        "    #adding into dictionary\r\n",
        "    invertedIndexing[word] = index\r\n",
        "    #calulating the word comes how many times that word comes in that document.\r\n",
        "    postingList[index] = dict(sorted(value.items(), key=lambda item: item[1], reverse=True))\r\n",
        "    #incrementing the index value\r\n",
        "    index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7NLw1BkRS9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5d68b3-422a-470e-fe64-de7282964d42"
      },
      "source": [
        "#listdir is used to list all the file\r\n",
        "files = os.listdir(\"/content/drive/My Drive/IRE/stories\")\r\n",
        "preprocessedData = preprocessing_data(files)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ab40thv.txt has processed  1\n",
            "aislesix.txt has processed  2\n",
            "18.lws has processed  3\n",
            "abbey.txt has processed  4\n",
            "alad10.txt has processed  5\n",
            "aquith.txt has processed  6\n",
            "aesop11.txt has processed  7\n",
            "altside.hum has processed  8\n",
            "5orange.txt has processed  9\n",
            "advtthum.txt has processed  10\n",
            "4moons.txt has processed  11\n",
            "16.lws has processed  12\n",
            "3gables.txt has processed  13\n",
            "13chil.txt has processed  14\n",
            "6ablemen.txt has processed  15\n",
            "assorted.txt has processed  16\n",
            "arctic.txt has processed  17\n",
            "adv_alad.txt has processed  18\n",
            "advsayed.txt has processed  19\n",
            "aluminum.hum has processed  20\n",
            "100west.txt has processed  21\n",
            "aisle.six has processed  22\n",
            "19.lws has processed  23\n",
            "7voysinb.txt has processed  24\n",
            "aminegg.txt has processed  25\n",
            "7oldsamr.txt has processed  26\n",
            "asop has processed  27\n",
            "alissadl.txt has processed  28\n",
            "6napolen.txt has processed  29\n",
            "bagel.man has processed  30\n",
            "17.lws has processed  31\n",
            "abyss.txt has processed  32\n",
            "20.lws has processed  33\n",
            "3student.txt has processed  34\n",
            "aircon.txt has processed  35\n",
            "3wishes.txt has processed  36\n",
            "archive has processed  37\n",
            "angelfur.hum has processed  38\n",
            "3lpigs.txt has processed  39\n",
            "14.lws has processed  40\n",
            "angry_ca.txt has processed  41\n",
            "aesopa10.txt has processed  42\n",
            "antcrick.txt has processed  43\n",
            "adler.txt has processed  44\n",
            "arcadia.sty has processed  45\n",
            "3sonnets.vrs has processed  46\n",
            "bestwish has processed  47\n",
            "blabnove.hum has processed  48\n",
            "bulphrek.txt has processed  49\n",
            "blasters.fic has processed  50\n",
            "blind.txt has processed  51\n",
            "bigred.hum has processed  52\n",
            "bulironb.txt has processed  53\n",
            "blh.txt has processed  54\n",
            "bgb.txt has processed  55\n",
            "breaks3.asc has processed  56\n",
            "bulolli1.txt has processed  57\n",
            "blak has processed  58\n",
            "bulmrx.txt has processed  59\n",
            "buldetal.txt has processed  60\n",
            "bulzork1.txt has processed  61\n",
            "beggars.txt has processed  62\n",
            "beast.asc has processed  63\n",
            "bookem2 has processed  64\n",
            "bluebrd.txt has processed  65\n",
            "bruce-p.txt has processed  66\n",
            "bran has processed  67\n",
            "batlslau.txt has processed  68\n",
            "bulwer.lytton has processed  69\n",
            "blue has processed  70\n",
            "beyond.hum has processed  71\n",
            "buggy.txt has processed  72\n",
            "breaks2.asc has processed  73\n",
            "bulnland.txt has processed  74\n",
            "bullove.txt has processed  75\n",
            "bulnoopt.txt has processed  76\n",
            "beautbst.txt has processed  77\n",
            "bumm has processed  78\n",
            "bulolli2.txt has processed  79\n",
            "bulfelis.txt has processed  80\n",
            "berternie.txt has processed  81\n",
            "bgcspoof.txt has processed  82\n",
            "bagelman.txt has processed  83\n",
            "blackrdr has processed  84\n",
            "bern has processed  85\n",
            "buldream.txt has processed  86\n",
            "brain.damage has processed  87\n",
            "blabnove.txt has processed  88\n",
            "breaks1.asc has processed  89\n",
            "blossom.pom has processed  90\n",
            "bookem3 has processed  91\n",
            "blackp.txt has processed  92\n",
            "bulprint.txt has processed  93\n",
            "bookem.1 has processed  94\n",
            "bram has processed  95\n",
            "bulhuntr.txt has processed  96\n",
            "bishop00.txt has processed  97\n",
            "crazy.hum has processed  98\n",
            "dskool.txt has processed  99\n",
            "burintrv.66 has processed  100\n",
            "clon has processed  101\n",
            "deal has processed  102\n",
            "descent.poe has processed  103\n",
            "cum has processed  104\n",
            "domain.poe has processed  105\n",
            "clevdonk.txt has processed  106\n",
            "cabin.txt has processed  107\n",
            "burintrv.92 has processed  108\n",
            "ccm.txt has processed  109\n",
            "day.in.mcdonald has processed  110\n",
            "cardcnt.txt has processed  111\n",
            "curious.george has processed  112\n",
            "disco.be.fun has processed  113\n",
            "elveshoe.txt has processed  114\n",
            "dopedenn.txt has processed  115\n",
            "darkness.txt has processed  116\n",
            "candle.hum has processed  117\n",
            "chik has processed  118\n",
            "confilct.fun has processed  119\n",
            "comp has processed  120\n",
            "dakota.txt has processed  121\n",
            "dicegame.txt has processed  122\n",
            "consumdr.hum has processed  123\n",
            "deathmrs.d has processed  124\n",
            "cameloto.hum has processed  125\n",
            "cow.exploder has processed  126\n",
            "elite.app has processed  127\n",
            "dtruck.txt has processed  128\n",
            "corcor.hum has processed  129\n",
            "campfire.txt has processed  130\n",
            "dwar has processed  131\n",
            "cmoutmou.txt has processed  132\n",
            "empnclot.txt has processed  133\n",
            "bureau.txt has processed  134\n",
            "deer.txt has processed  135\n",
            "cooldark.txt has processed  136\n",
            "burintrv.78 has processed  137\n",
            "charlie.txt has processed  138\n",
            "dan has processed  139\n",
            "cybersla.txt has processed  140\n",
            "burltrs has processed  141\n",
            "dicksong.txt has processed  142\n",
            "diaryflf.txt has processed  143\n",
            "crabhern.txt has processed  144\n",
            "contrad1.hum has processed  145\n",
            "cooldark.sto has processed  146\n",
            "discocanbefun.txt has processed  147\n",
            "emperor3.txt has processed  148\n",
            "burn has processed  149\n",
            "freeman.fil has processed  150\n",
            "enc has processed  151\n",
            "fish.txt has processed  152\n",
            "empsjowk.txt has processed  153\n",
            "fran has processed  154\n",
            "ghost has processed  155\n",
            "encamp01.txt has processed  156\n",
            "flute.txt has processed  157\n",
            "foxngrap.txt has processed  158\n",
            "girl has processed  159\n",
            "foxnstrk.txt has processed  160\n",
            "fantas.hum has processed  161\n",
            "fic5 has processed  162\n",
            "ezoff has processed  163\n",
            "fearmnky has processed  164\n",
            "fantasy.txt has processed  165\n",
            "fic4 has processed  166\n",
            "floc has processed  167\n",
            "game.txt has processed  168\n",
            "fable.txt has processed  169\n",
            "floobs.txt has processed  170\n",
            "fantasy.hum has processed  171\n",
            "enya_trn.txt has processed  172\n",
            "eyeargon.hum has processed  173\n",
            "fear.hum has processed  174\n",
            "gatherng.txt has processed  175\n",
            "flktrp.txt has processed  176\n",
            "enginer.txt has processed  177\n",
            "fic1 has processed  178\n",
            "fred.txt has processed  179\n",
            "fowl.death has processed  180\n",
            "fic2 has processed  181\n",
            "fic3 has processed  182\n",
            "frogp.txt has processed  183\n",
            "empty.txt has processed  184\n",
            "fgoose.txt has processed  185\n",
            "excerpt.txt has processed  186\n",
            "gay has processed  187\n",
            "fea1 has processed  188\n",
            "forgotte has processed  189\n",
            "fleas.txt has processed  190\n",
            "frum has processed  191\n",
            "flytrunk.txt has processed  192\n",
            "fea3 has processed  193\n",
            "foxncrow.txt has processed  194\n",
            "fic7 has processed  195\n",
            "friend.s has processed  196\n",
            "fourth.fic has processed  197\n",
            "friends.txt has processed  198\n",
            "gemdra.txt has processed  199\n",
            "girlclub.txt has processed  200\n",
            "fea2 has processed  201\n",
            "excerpt.hum has processed  202\n",
            "enchdup.hum has processed  203\n",
            "hansgrtl.txt has processed  204\n",
            "horswolf.txt has processed  205\n",
            "igiv has processed  206\n",
            "goldenp.txt has processed  207\n",
            "how.ernie.bert has processed  208\n",
            "gloves.txt has processed  209\n",
            "jackbstl.txt has processed  210\n",
            "gold3ber.txt has processed  211\n",
            "graymare.txt has processed  212\n",
            "goldfish.txt has processed  213\n",
            "glimpse1.txt has processed  214\n",
            "holmesbk.txt has processed  215\n",
            "imagin.hum has processed  216\n",
            "greedog.txt has processed  217\n",
            "goldbug.poe has processed  218\n",
            "hotline4.txt has processed  219\n",
            "island.poe has processed  220\n",
            "greatlrn.leg has processed  221\n",
            "hellmach.txt has processed  222\n",
            "gulliver.txt has processed  223\n",
            "haretort.txt has processed  224\n",
            "hareleph.txt has processed  225\n",
            "jackmac.fic has processed  226\n",
            "hell4.txt has processed  227\n",
            "hitch2.txt has processed  228\n",
            "idi.hum has processed  229\n",
            "inter has processed  230\n",
            "hound-b.txt has processed  231\n",
            "helmfuse.txt has processed  232\n",
            "goldgoos.txt has processed  233\n",
            "hotline3.txt has processed  234\n",
            "hotline1.txt has processed  235\n",
            "jaynejob.asc has processed  236\n",
            "hitch3.txt has processed  237\n",
            "hils has processed  238\n",
            "grav has processed  239\n",
            "hop-frog.poe has processed  240\n",
            "history5.txt has processed  241\n",
            "hareporc.txt has processed  242\n",
            "imonly17.txt has processed  243\n",
            "immorti.hum has processed  244\n",
            "healer.txt has processed  245\n",
            "horsdonk.txt has processed  246\n",
            "hole2nar.txt has processed  247\n",
            "immortal has processed  248\n",
            "home.fil has processed  249\n",
            "jerichms.hum has processed  250\n",
            "lionmane.txt has processed  251\n",
            "lpeargrl.txt has processed  252\n",
            "knuckle.txt has processed  253\n",
            "missing.txt has processed  254\n",
            "mattress.txt has processed  255\n",
            "mike.txt has processed  256\n",
            "kharian.txt has processed  257\n",
            "mouslion.txt has processed  258\n",
            "mydream.txt has processed  259\n",
            "lionmosq.txt has processed  260\n",
            "narciss.txt has processed  261\n",
            "lure.txt has processed  262\n",
            "lrrhood.txt has processed  263\n",
            "nigel.3 has processed  264\n",
            "nigel.1 has processed  265\n",
            "kneeslapper.txt has processed  266\n",
            "mcdonaldl.txt has processed  267\n",
            "ltp has processed  268\n",
            "mindprob.txt has processed  269\n",
            "mindwar has processed  270\n",
            "lionbird has processed  271\n",
            "nigel.10 has processed  272\n",
            "lmermaid.txt has processed  273\n",
            "ladylust.hum has processed  274\n",
            "musgrave.txt has processed  275\n",
            "mtinder.txt has processed  276\n",
            "myeyes has processed  277\n",
            "kneeslapper has processed  278\n",
            "mazarin.txt has processed  279\n",
            "monkking.txt has processed  280\n",
            "lil has processed  281\n",
            "lmtchgrl.txt has processed  282\n",
            "monksol.txt has processed  283\n",
            "modemhippy.txt has processed  284\n",
            "nigel.2 has processed  285\n",
            "korea.s has processed  286\n",
            "lgoldbrd.txt has processed  287\n",
            "musibrem.txt has processed  288\n",
            "mario.txt has processed  289\n",
            "lament.txt has processed  290\n",
            "jim.asc has processed  291\n",
            "melissa.txt has processed  292\n",
            "luf has processed  293\n",
            "kzap.txt has processed  294\n",
            "lionwar.txt has processed  295\n",
            "long1-3.txt has processed  296\n",
            "keepmodu.txt has processed  297\n",
            "keeping.insanit has processed  298\n",
            "life.txt has processed  299\n",
            "nitepeek.sto has processed  300\n",
            "nigel.5 has processed  301\n",
            "quarter.c16 has processed  302\n",
            "nigel.6 has processed  303\n",
            "paul_har.sto has processed  304\n",
            "progx has processed  305\n",
            "quarter.c14 has processed  306\n",
            "qcarroll has processed  307\n",
            "partya.txt has processed  308\n",
            "pregn.txt has processed  309\n",
            "nihgel_8.9 has processed  310\n",
            "quarter.c13 has processed  311\n",
            "quarter.c17 has processed  312\n",
            "non3 has processed  313\n",
            "perf has processed  314\n",
            "prince.art has processed  315\n",
            "non4 has processed  316\n",
            "non2 has processed  317\n",
            "quarter.c15 has processed  318\n",
            "quarter.c1 has processed  319\n",
            "paink-ws.txt has processed  320\n",
            "pepsi.degenerat has processed  321\n",
            "quarter.c19 has processed  322\n",
            "psi has processed  323\n",
            "outcast.dos has processed  324\n",
            "poem-2.txt has processed  325\n",
            "plescopm.txt has processed  326\n",
            "quarter.c10 has processed  327\n",
            "poem-4.txt has processed  328\n",
            "pepdegener.txt has processed  329\n",
            "poplstrm.txt has processed  330\n",
            "quarter.c18 has processed  331\n",
            "parotsha.txt has processed  332\n",
            "panama.txt has processed  333\n",
            "quarter.c12 has processed  334\n",
            "pussboot.txt has processed  335\n",
            "piracy.sto has processed  336\n",
            "poem-1.txt has processed  337\n",
            "quarter.c11 has processed  338\n",
            "obstgoat.txt has processed  339\n",
            "peace.fun has processed  340\n",
            "pinocch.txt has processed  341\n",
            "psf.txt has processed  342\n",
            "psyc has processed  343\n",
            "oxfrog.txt has processed  344\n",
            "nigel.7 has processed  345\n",
            "nigel.4 has processed  346\n",
            "omarsheh.txt has processed  347\n",
            "pphamlin.txt has processed  348\n",
            "roger1.txt has processed  349\n",
            "quest has processed  350\n",
            "quot has processed  351\n",
            "shrdfarm.txt has processed  352\n",
            "sre-dark.txt has processed  353\n",
            "s&m_plot has processed  354\n",
            "stainles.ana has processed  355\n",
            "solitary.txt has processed  356\n",
            "snowqn1.txt has processed  357\n",
            "sqzply.txt has processed  358\n",
            "spider.txt has processed  359\n",
            "rainda.txt has processed  360\n",
            "quarter.c8 has processed  361\n",
            "sick-kid.txt has processed  362\n",
            "quarter.c6 has processed  363\n",
            "shoscomb.txt has processed  364\n",
            "spiders.txt has processed  365\n",
            "quarter.c7 has processed  366\n",
            "safe has processed  367\n",
            "startrek.txt has processed  368\n",
            "robotech has processed  369\n",
            "snow.txt has processed  370\n",
            "radar_ra.txt has processed  371\n",
            "quickfix has processed  372\n",
            "spectacl.poe has processed  373\n",
            "snowmaid.txt has processed  374\n",
            "sanpedr2.txt has processed  375\n",
            "reap has processed  376\n",
            "sleprncs.txt has processed  377\n",
            "redragon.txt has processed  378\n",
            "rocket.sf has processed  379\n",
            "quarter.c9 has processed  380\n",
            "running.txt has processed  381\n",
            "space.txt has processed  382\n",
            "quarter.c3 has processed  383\n",
            "reality.txt has processed  384\n",
            "social.vikings has processed  385\n",
            "socialvikings.txt has processed  386\n",
            "retrib.txt has processed  387\n",
            "rock has processed  388\n",
            "stairdre.txt has processed  389\n",
            "shulk.txt has processed  390\n",
            "silverb.txt has processed  391\n",
            "spam.key has processed  392\n",
            "quarter.c4 has processed  393\n",
            "sight.txt has processed  394\n",
            "rid.txt has processed  395\n",
            "sis has processed  396\n",
            "quarter.c5 has processed  397\n",
            "quarter.c2 has processed  398\n",
            "s&m_that has processed  399\n",
            "timetrav.txt has processed  400\n",
            "tao3.dos has processed  401\n",
            "wlgirl.txt has processed  402\n",
            "times.fic has processed  403\n",
            "wolf7kid.txt has processed  404\n",
            "veiledl.txt has processed  405\n",
            "tcoa.txt has processed  406\n",
            "taxnovel.txt has processed  407\n",
            "vday.hum has processed  408\n",
            "valen has processed  409\n",
            "wolfcran.txt has processed  410\n",
            "wanderer.fun has processed  411\n",
            "vampword.txt has processed  412\n",
            "timem.hac has processed  413\n",
            "testpilo.hum has processed  414\n",
            "toilet.s has processed  415\n",
            "wall.art has processed  416\n",
            "sunday.txt has processed  417\n",
            "szechuan has processed  418\n",
            "superg1 has processed  419\n",
            "tearglas.txt has processed  420\n",
            "unluckwr.txt has processed  421\n",
            "weaver.txt has processed  422\n",
            "t_zone.jok has processed  423\n",
            "whgdsreg.reg has processed  424\n",
            "tin has processed  425\n",
            "terrorbears.txt has processed  426\n",
            "traitor.txt has processed  427\n",
            "tctac.txt has processed  428\n",
            "vaincrow.txt has processed  429\n",
            "thewave has processed  430\n",
            "tinsoldr.txt has processed  431\n",
            "wisteria.txt has processed  432\n",
            "sucker.txt has processed  433\n",
            "tuc_mees has processed  434\n",
            "withdraw.cyb has processed  435\n",
            "vainsong.txt has processed  436\n",
            "tailbear.txt has processed  437\n",
            "textfile.primer has processed  438\n",
            "weeprncs.txt has processed  439\n",
            "thanksg has processed  440\n",
            "the-tree.txt has processed  441\n",
            "vgilante.txt has processed  442\n",
            "telefone.txt has processed  443\n",
            "uglyduck.txt has processed  444\n",
            "stsgreek has processed  445\n",
            "tree.txt has processed  446\n",
            "wolflamb.txt has processed  447\n",
            "wrt has processed  448\n",
            "sre07.txt has processed  449\n",
            "sre02.txt has processed  450\n",
            "sre04.txt has processed  451\n",
            "sre05.txt has processed  452\n",
            "sre_feqh.txt has processed  453\n",
            "sre01.txt has processed  454\n",
            "sre06.txt has processed  455\n",
            "yukon.txt has processed  456\n",
            "sre08.txt has processed  457\n",
            "sre_finl.txt has processed  458\n",
            "srex.txt has processed  459\n",
            "sre10.txt has processed  460\n",
            "sre03.txt has processed  461\n",
            "sre_sei.txt has processed  462\n",
            "wombat.und has processed  463\n",
            "zombies.txt has processed  464\n",
            "sre09.txt has processed  465\n",
            "write has processed  466\n",
            "sretrade.txt has processed  467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG39kAgzBV3n"
      },
      "source": [
        "import json\r\n",
        "#creating a json file for storing the preprocessed data\r\n",
        "with open('/content/drive/My Drive/IRE/preprocessedData.json', 'w') as fp:\r\n",
        "    json.dump(preprocessedData, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXdbWglSdyx0"
      },
      "source": [
        "import json \r\n",
        "preprocessedData = {}\r\n",
        "#load the preprocessedData.json file for Indexing\r\n",
        "with open('/content/drive/My Drive/IRE/preprocessedData.json') as json_file: \r\n",
        "\tpreprocessedData = json.load(json_file) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1kNtOh2BH2p"
      },
      "source": [
        "#Indexing of preprocessed data\r\n",
        "for key, value in preprocessedData.items():\r\n",
        "  createIndexing(value, key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArsaOiGc8YxL"
      },
      "source": [
        "#Inverted Index \r\n",
        "createInvertedIndexing()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gg8zM3f5Ymtd"
      },
      "source": [
        "import json\r\n",
        "#Saving the indexing.json,invertedIndexing.json and postinglist.json file\r\n",
        "with open('/content/drive/My Drive/IRE/indexing.json', 'w') as fp:\r\n",
        "    json.dump(indexing, fp)\r\n",
        "with open('/content/drive/My Drive/IRE/invertedIndexing.json', 'w') as fp:\r\n",
        "    json.dump(invertedIndexing, fp)\r\n",
        "with open('/content/drive/My Drive/IRE/postinglist.json', 'w') as fp:\r\n",
        "    json.dump(postingList, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-1iRpirRYnU"
      },
      "source": [
        "#reading the titles_stories.txt file\r\n",
        "tiles = open('/content/drive/My Drive/IRE/titles_stories.txt',mode = 'r')\r\n",
        "Titles = tiles.read()\r\n",
        "Titles_dict = ast.literal_eval(Titles) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F5NbD8N5ZBJ"
      },
      "source": [
        "with open('/content/drive/My Drive/IRE/title_indexing.json', 'w') as fp:\r\n",
        "    json.dump(Titles_dict, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiNCovLhxmpd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bdd393ee-5da5-43d4-807c-3ff6450ab563"
      },
      "source": [
        "Titles_dict['lmermaid.txt']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The Fable of the Little Mermaid'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFNn11m55N18"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}